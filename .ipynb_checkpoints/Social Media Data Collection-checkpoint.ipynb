{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping 100 recipes\n",
    "\n",
    "Below section illustrates how to extract 100 recipes from food section on the ndtv.com website. We extract 52 recipes from the vegetarian recipes section and about 48 recipes from the healthy recipes section on the ndtv.com website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Type: VEGETARIAN Recipes\n"
     ]
    }
   ],
   "source": [
    "# Extract vegetarian recipes\n",
    "url = 'https://food.ndtv.com/recipes/vegetarian-recipes'\n",
    "html_doc = requests.get(url).content\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the source\n",
    "recipe_container = soup.find(\"div\", {\"class\": \"recp-det-cont\"})\n",
    "source1 = recipe_container.find('h1').get_text().strip()\n",
    "print('Recipe Type:',source1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the inidvidual components: name, url and ingredients for each recipe\n",
    "# name\n",
    "recipe_ingredients = recipe_container.find('div', {\"id\": \"recipeListing\"})\n",
    "veg_recipe_names = [x.get_text().strip()\n",
    "               for x in recipe_ingredients.find_all('a')]\n",
    "# links\n",
    "recipe_ingredients = recipe_container.find('div', {\"id\": \"recipeListing\"})\n",
    "veg_recipe_links = [x.get('href')\n",
    "               for x in recipe_ingredients.find_all('a')]\n",
    "\n",
    "#ingredients\n",
    "veg_recipe_ingredients = []\n",
    "remove = []\n",
    "for link in veg_recipe_links:\n",
    "    url = link\n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    recipe_container = soup.find(\"div\", {\"class\": \"recp-det-cont\"})\n",
    "    recipe_ingredients = recipe_container.find('div', {\"class\": \"ingredients\"})\n",
    "    \n",
    "    # Remove extra subtitles from the ingredients\n",
    "    unwanted = recipe_container.find('div', {\"class\": \"ingredients\"}).find_all('b')\n",
    "    for x in unwanted:\n",
    "        remove.append(x.get_text().strip())\n",
    "        \n",
    "    veg_recipe_ingredients.append([y.get_text().strip() for y in recipe_ingredients.find_all('li') \n",
    "                         if y.get_text().strip() not in remove and not(y.get_text().strip().startswith('For')) and not(y.get_text().strip().startswith('Ingredients'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract healthly recipes\n",
    "url = 'https://food.ndtv.com/recipes/meat-recipes'\n",
    "html_doc = requests.get(url).content\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the source\n",
    "recipe_container = soup.find(\"div\", {\"class\": \"recp-det-cont\"})\n",
    "source2 = recipe_container.find('h1').get_text().strip()\n",
    "print('Recipe Type:',source2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the inidvidual components: name, url and ingredients for each recipe\n",
    "# name\n",
    "recipe_ingredients = recipe_container.find('div', {\"id\": \"recipeListing\"})\n",
    "healthy_recipe_names = [x.get_text().strip()\n",
    "               for x in recipe_ingredients.find_all('a')]\n",
    "\n",
    "# links\n",
    "recipe_ingredients = recipe_container.find('div', {\"id\": \"recipeListing\"})\n",
    "healthy_recipe_links = [x.get('href')\n",
    "               for x in recipe_ingredients.find_all('a')]\n",
    "\n",
    "#ingredients\n",
    "healthy_recipe_ingredients = []\n",
    "remove = []\n",
    "for link in healthy_recipe_links:\n",
    "    url = link\n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    recipe_container = soup.find(\"div\", {\"class\": \"recp-det-cont\"})\n",
    "    recipe_ingredients = recipe_container.find('div', {\"class\": \"ingredients\"})\n",
    "    \n",
    "     # Remove extra subtitles from the ingredients\n",
    "    unwanted = recipe_container.find('div', {\"class\": \"ingredients\"}).find_all('b')    \n",
    "    for x in unwanted:\n",
    "        remove.append(x.get_text().strip())\n",
    "        \n",
    "    healthy_recipe_ingredients.append([y.get_text().strip() for y in recipe_ingredients.find_all('li') \n",
    "                         if y.get_text().strip() not in remove and not(y.get_text().strip().startswith('For')) and not(y.get_text().strip().startswith('Ingredients'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the list of health recipes and vegetarian recipes\n",
    "for i in range(len(healthy_recipe_names)):\n",
    "    if len(veg_recipe_names) < 100:\n",
    "        veg_recipe_names.append(healthy_recipe_names[i])\n",
    "        veg_recipe_links.append(healthy_recipe_links[i])\n",
    "        veg_recipe_ingredients.append(healthy_recipe_ingredients[i])\n",
    "\n",
    "# Creating a data dictionary for all the veg and healthy recipes\n",
    "data = {'url':veg_recipe_links,'name':veg_recipe_names,'ingredient':veg_recipe_ingredients}\n",
    "\n",
    "raw_data = pd.DataFrame(data=data)\n",
    "raw_data_final = raw_data.set_index(['url','name'])['ingredient'].apply(lambda x: pd.Series(x)).stack().reset_index(level=2, drop=True).to_frame('ingredient')\n",
    "raw_data_final = raw_data_final.reset_index()\n",
    "raw_data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the data to csv file\n",
    "raw_data_final.to_csv('./a2_rawData.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned the Data\n",
    "\n",
    "In this section, we illustrate how to clean the ingredients data from the ndtv.com website. We look for additonal white spaces, removing measure metrics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import pattern\n",
    "from pattern.text.en import singularize\n",
    "\n",
    "# Copy the dataframe into a new dataframe\n",
    "clean_data_intermediate = raw_data_final\n",
    "clean_data_intermediate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the data\n",
    "pattern = '[0-9]|[^A-Za-z]+' # patterns to be removed\n",
    "# Extra words to be removed\n",
    "remove_words = ['gm','gms','gram','grams','kg','tbsp','tsp','ml','cm','cups','pcs','cup','inch','height','drops','litre','piece','chotti','adding','whisk','well','packed',\n",
    "                'pinch','per','taste','peeled','little','cook','optional','garnishing','finely','chopped','unsweetened','rajasthani','elaichi','compound','bruised',\n",
    "                'freshly','uncooked','toasted','powdered','required','cube','cut','pieces','sliced','minced','shredded','glasses','pair','squeezed','lavang', 'fairly',\n",
    "                'shell','melted','few','stranded','grated','strands','liquid','cubes','refined','dessicated','sultanas','cleaned','washed','ground','jaiphal','approx','each',\n",
    "                'fresh','pinches','frying','clarified','few','purpose','tulsi','crushed','fried','pre','soaked','softened','shahi','zeera','pasanda','thin','slice',\n",
    "                'christmas','fry','segments','smashed','roasted','large','nos','granulated','ripened','diced','crumbled','five','marinade','pounded','javitri','crush',\n",
    "                'whisked','cubed','mashed','glass','refined','coarse','grind','enough','regular','extra','fat','skimmed','spicy','desi','ground','and','lukewarm','dalia',\n",
    "                'small','squares','seedless','balls','srushed','skim','atta','frozen','dried','full','shakes','preferably','it','like','style','deseeded','approximately',\n",
    "                'slices','cooled','medium','roll','to','before','soaking','as','sendha','namak','sweetening','agent','healthier','you','curry','slit','slightly','grilled'\n",
    "                'substitute','halved','khaand','low','boiled','a','holy','purchased','homemade','dusting','fine','work','seedless','if','cleaned','kaalimirch','seasoning',\n",
    "                'defrosted','young','season','deep','quartered','chargrilled','whole','spices','himalayan','best','rolling','cooked','or','coarsely','ready','add',\n",
    "                'grated','drained','kashmiri','garnish','handful','according','wash','very','half','day','one','bowl','mixed','spicy','dipping','coarsely']\n",
    "ingredient_new = []\n",
    "rx = re.compile(r'\\ban\\b|\\bthe\\b|\\band\\b|\\bfor\\b|\\ba\\b|\\bin\\b|\\ball\\b|\\bas\\b|\\bfrom\\b|\\bof\\b|\\bto\\b|\\bfor\\b|\\binto\\b|\\bas\\b\\|\\bor\\b')\n",
    "\n",
    "# Loop through all words and clean\n",
    "for ingredient in clean_data_intermediate['ingredient'].values:\n",
    "    ingredient_nw = re.sub(pattern,' ', ingredient)\n",
    "    #print(ingredient_nw)\n",
    "    ingredient_nw = rx.sub(' ', ingredient_nw)\n",
    "    \n",
    "    ingredient_new.append(ingredient_nw)\n",
    "    \n",
    "#ingredient_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all words and clean\n",
    "ingredient_cleaned = []\n",
    "for ingredient in ingredient_new:\n",
    "    for word in ingredient:\n",
    "        res = ingredient.split()\n",
    "\n",
    "        res_final = [res[i].lower() for i in range(len(res)) if res[i].lower() not in remove_words]\n",
    "        res_final = ' '.join(res_final)\n",
    "        res_final = res_final.strip()\n",
    "    ingredient_cleaned.append(res_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning additional data points in ingredient column\n",
    "clean_data_intermediate['ingredient_cleaned'] = ingredient_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning long sentences\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'red chilli powder cayenne pepper'] = 'red chilli powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'red rice vermicelli broken sooji semolina vermicelli'] = 'vermicelli'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'olive oil not virgin'] = 'olive oil'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'mixed nuts almonds cashews walnuts pistachios peanuts'] = 'mixed nuts'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'raisins cranberries citrus peel nuts' ] = 'nuts'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'mix black pepper cinnamon cardamom clove bay leaf'] = 'spices'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'red yellow green bell peppers'] = 'bell pepper'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'urad dal flour dry roast dal powder'] = 'urad dal powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'coconut oil vegetable oil'] = 'coconut oil'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'herbs basil parsley thyme cilantro'] = 'herbs'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'parmesan mozzarella cheese'] = 'parmesan cheese'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'mutton clean male goat thigh'] = 'mutton'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'mash ki dal urad dal'] = 'urad dal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning long sentences\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'meat chicken mutton turkey lamb'] = 'meat'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'spice mix turmeric red chili powder coriander powder cumin powder salt garam masala made paste with water'] = 'spice'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'duck vertically'] = 'duck'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'processed cheese parmesan cheese'] = 'parmesan cheese'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'cloves green cardamoms pepper corns tiny cinnamon together'] = 'condiment'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'tamarind sauce with dash chilli sauce vinegar'] = 'sauce'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'sized prawns shelled de veined'] = 'prawn'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'stalk spring onion both white green portion'] = 'spring onion'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'coconut poqder'] = 'coconut powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'tomatoes skewers oil brush chaat masala'] = 'tomatoes'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'saltand pepper'] = 'salt and pepper'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'] == 'tomato de skinned de seeded'] = 'tomato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing similar words with the same word\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['coriander','green coriander','coriander green','coriander leaves','coriander leaf'])] = 'coriander'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['flour maida'])] = 'maida'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['green chillies','chili green'])] = 'green chilli'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['black pepper','pepper','ground black pepper','black pepper powder','peppers'])] = 'black pepper'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['basil','basil leaves'])] = 'basil'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['potatoes','potato'])] = 'potato'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['onions','onion','onions size'])] = 'onion'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['apples','apple'])] = 'apple'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['almonds','almond'])] = 'almond'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['cloves','clove'])] = 'clove'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['bread crumbs','bread crumb','breadcrumbs'])] = 'bread crumbs'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['cashews','cashew nuts','cashewnuts','cashew nut','blend with water make paste cashew'])] = 'cashews'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['chilly flakes','chilli flakes','chili flakes'])] = 'chilli flakes'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['garlic cloves','garlic clove','cloves garlic','garlic','garlic pods'])] = 'garlic'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['veg stock','vegetable stock'])] = 'vegetable stock'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['straberries','strawberry'])] = 'strawberry'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['straberries pureed','strawberry puree'])] = 'strawberry puree'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['lemon juiced','lemon juice'])] = 'lemon juice'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['jaggery gur','jaggery organic','jaggery'])] = 'jaggery'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['sprig curry leaves','sprigs curry leaves'])] = 'sprig curry leaves'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['raspeberry','raspberries'])] = 'raspberry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing similar words with the same word\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['gulaabjal','rose water gulaabjal'])] = 'rose water'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['bayleaf','bay leaf','bay leaves'])] = 'bayleaf'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['soak tablespoon water saffron','saffron diluted water'])] = 'saffron water'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['powder garam masala','garam masala'])] = 'garam masala powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['black peppercorns','peppercorn','black pepper corn','black peppercorn','black pepper corns','black pepper corns'])] = 'black pepper corn'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['cinnamon sticks','stick cinnamon','cinnamon','cinnamon sticks'])] = 'cinnamon stick'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['chilli powder','chili powder'])] = 'chilli powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['egg','eggs'])] = 'egg'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['red chili powder','red chilli powder','red chillli powder'])] = 'red chilli powder'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['brown onion','brown onions'])] = 'brown onion'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['bell pepper','bell peppers'])] = 'bell pepper'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['black cardamom','black cardamoms','cardamom','cardamoms','cardamoms pods'])] = 'cardamom'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['carrot','carrots'])] = 'carrot'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['mint leaf','mint leaves'])] = 'mint leaves'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['lemon juice','lemon juice lemons'])] = 'lemon'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['mustard oil','mustard oil oil'])] = 'mustard oil'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['malted vinegar','malt vinegar'])] = 'malt vinegar'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['green cardamoms','green cardamom'])] = 'green cardamom'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['cornflour','corn flour'])] = 'corn flour'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['cashew nut paste','cashew paste'])] = 'cashew paste'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['yogurt beaten','yogurt dahi','yogurt plain','yoghurt','yogurt'])] = 'yogurt'\n",
    "clean_data_intermediate['ingredient_cleaned'].loc[clean_data_intermediate['ingredient_cleaned'].isin(['green chili','green chilli','green chily','green chilies'])] = 'green chillies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "clean_data_intermediate.head()\n",
    "clean_data_intermediate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ingredients data points which are not ingredients\n",
    "remove_non_ingredients = ['oven proof width','food thermometer','plastic bag','paste water','japanese','lasagna sheets','serve','banana leaf covering leg lamb','']\n",
    "clean_data_final = clean_data_intermediate[~clean_data_intermediate['ingredient_cleaned'].isin(remove_non_ingredients)]\n",
    "clean_data_final = pd.DataFrame(data=clean_data_final,columns=['url','name','ingredient_cleaned'])\n",
    "clean_data_final.rename(columns={'ingredient_cleaned':'ingredient'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract to csv\n",
    "clean_data_final.drop_duplicates(inplace=True)\n",
    "clean_data_final.to_csv('./a2_cleanData.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Result Analysis\n",
    "\n",
    "This section illustrates how to calculate count and proportion for each individual ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cpy the cleaned data file\n",
    "recipe_data_analysis = clean_data_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each ingredient\n",
    "recipe_data = recipe_data_analysis.groupby('ingredient')['name'].count()\n",
    "recipe_data.sort_values(ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe\n",
    "recipe_data_analysis = {'ingredient':recipe_data.index,'count':recipe_data.values,'proportion':recipe_data.values/len(set(clean_data_final['name']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the top most common ingredients\n",
    "recipe_data_analysis = pd.DataFrame(recipe_data_analysis)\n",
    "recipe_data_analysis = recipe_data_analysis.head(10)\n",
    "recipe_data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract to csv\n",
    "recipe_data_analysis.to_csv('./a2_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
